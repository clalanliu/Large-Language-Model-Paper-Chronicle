# Large-Language-Model-Paper-Chronicle

## Large Language Model
### 2022
[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)
[BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)

### 2021
[PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)
[Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446)

### 2020
[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://openreview.net/forum?id=H1eA7AEtvS)  
[T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
[GPT3: Language Models Are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

### 2019
[GPT2: Language Models Are Unsupervised Multitask Learners](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask)  
[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)  
[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)  

### 2018
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)  



